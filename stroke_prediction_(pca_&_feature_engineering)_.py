# -*- coding: utf-8 -*-
"""Stroke Prediction (PCA & Feature Engineering) .ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/168GNRE_5qjOIq7Z5jYQ-ss7befNfMfS7
"""

from google.colab import drive
drive.mount ('/content/drive')

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

#Seaborn
import seaborn as sns
from seaborn import heatmap
 
#Keras & TensorFlow
import tensorflow
from tensorflow import keras as keras
from keras.models import Sequential
from keras.layers import Dense, Dropout
from keras.callbacks import EarlyStopping
from keras import metrics
 
# Sci-kit learn
from sklearn.metrics import classification_report, ConfusionMatrixDisplay,\
 mean_squared_error, r2_score, mean_absolute_error
from sklearn.preprocessing import StandardScaler, OneHotEncoder, MinMaxScaler
from sklearn.compose import make_column_transformer, make_column_selector
from sklearn.model_selection import train_test_split
from sklearn.impute import SimpleImputer
from sklearn.pipeline import make_pipeline
from sklearn.decomposition import PCA

#KMeans Clustering 

from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import accuracy_score, recall_score, precision_score, \
f1_score, classification_report, confusion_matrix
from sklearn.metrics import ConfusionMatrixDisplay, RocCurveDisplay
from sklearn import metrics
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.neighbors import KNeighborsClassifier

stroke = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Coding Bootcamp/Project 2 /healthcare-dataset-stroke-data.csv')
df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Coding Bootcamp/Project 2 /healthcare-dataset-stroke-data.csv')
stroke.head(2)

df.info()

"""Feature Engineering"""

df['Diseases Out of 3'] = df['stroke'] + df['hypertension'] + df['heart_disease']
df['Diseases Out of 3'].value_counts()

df["Obesity"] = df["bmi"] * df["avg_glucose_level"] / 1000
df["Obesity"]

df['smoking_status'].value_counts()

df['ever_married'].value_counts()

df['Married & Stroke'] = []
if df['ever_married'] == 'Yes' :

df.head()

#Checking the column 'bmi' because it was the only one contaning missing values
stroke['bmi'].nunique()

#stroke.drop(stroke.loc[stroke['gender']== 'Other'].index, inplace=True)
#stroke = stroke[stroke.'gender' != 'Other']
stroke = stroke[stroke["gender"].str.contains("Other") == False]

stroke['gender'].value_counts()

nominal_selector = stroke.select_dtypes( include='object')
for col in nominal_selector.columns:
  print(col)
  print(nominal_selector[col].value_counts(), '\n')

stroke['ever_married'].value_counts()

stroke.describe()

"""Average Glucose level is one of the few abnormla distributions - this one is skewed to the right. BMI is also a histogram that poses a slight skew to the right. Stroke, Heart Disease and Hypertension are all bimodal since the columns are """

#Creating a filter to select all columns that are non-numerical so we can correct for inconsistencies
nominal_selector = stroke.select_dtypes(include='object')
for col in nominal_selector.columns:
  print(col)
  print(nominal_selector[col].value_counts(), '\n');

"""Validation Split - Classification Problem"""

#Assigning and splitting target column to X and y with random state of 42

X=stroke.drop(columns='stroke')
y = stroke['stroke']
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)

nominal_selector = make_column_selector(dtype_include='object')
numeric_selector = make_column_selector(dtype_include='number')

#Creating our imputer for later inputing them into our preprocessor and pipeline

scaler = StandardScaler()
ohe = OneHotEncoder(sparse=False, handle_unknown='ignore')

mean_imputer = SimpleImputer(strategy='mean')
missing_imputer = SimpleImputer(strategy='constant', fill_value='Missing')

nominal_pipeline = make_pipeline(missing_imputer, ohe)
numeric_pipeline = make_pipeline(mean_imputer, scaler)

nominal_tup = (nominal_pipeline, (['gender', 'ever_married', 'work_type', 'Residence_type', 'smoking_status']))
numeric_tup = (numeric_pipeline, numeric_selector)

preprocessor = make_column_transformer(numeric_tup, nominal_tup, remainder='passthrough')

preprocessor.fit(X_train)

"""The number of people who have suffered from stroke are very minimal compared to those who have had a stroke. """

#Creating a function to evaluate metrics towards prediction

def evaluate_classification(model, X_test, y_test, cmap='Greens',
                            normalize=None, classes=None, figsize=(20,5)):
  """Takes as arguments: a model, features, and labels
  Prints a classification report, confusion matrix
  Optional arguments: 
    cmap: colormap 
    normalize: confusion matrix normalization ['true', 'pred', 'all' or None]
    classes: ordered list of class labels
    figsize: size of figure"""
    
  test_preds = model.predict(X_test)
  print(metrics.classification_report(y_test, test_preds, target_names=classes))
  
  ConfusionMatrixDisplay.from_estimator(model, X_test, y_test, cmap=cmap, 
                                display_labels=classes,
                                normalize=normalize)
  plt.show()

# Commented out IPython magic to ensure Python compatibility.
# %%time
# logreg = LogisticRegression()
# logreg_pipe = make_pipeline(preprocessor, logreg)
# logreg_pipe.fit(X_train, y_train)

print(f'Training Score:', logreg_pipe.score(X_train, y_train))
print(f' Testing Score:', logreg_pipe.score(X_test, y_test))

#L1 Tunning
c_valuesl1 = [0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000]
train_scoresl1 = []
test_scoresl1 = []
for c in c_valuesl1: 
  logregl1 = LogisticRegression(C=c, max_iter=1000, solver='liblinear', penalty='l1')
  logregl1_pipe = make_pipeline(preprocessor, logregl1)
  logregl1_pipe.fit(X_train, y_train)
  train_scoresl1.append(logregl1_pipe.score(X_train, y_train))
  test_scoresl1.append(logregl1_pipe.score(X_test, y_test))
{c:score for c, score in zip(c_valuesl1, test_scoresl1)}

fig, ax = plt.subplots(1,1)
ax.plot(c_valuesl1, train_scoresl1, label='Training Accuracy')
ax.plot(c_valuesl1, test_scoresl1, label='Testing Accuracy')
ax.set_xticks(c_valuesl1)
ax.set_title('Change in accuracy over C values for l1 regularization')
ax.legend()

#L2 Tunning
c_values2 = [0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000]
train_scores2 = []
test_scores2 = []
for c in c_values2: 
  logreg_l2 = LogisticRegression(C=c, max_iter=1000, solver='liblinear', penalty='l2')
  logreg_l2_pipe = make_pipeline(preprocessor, logreg)
  logreg_l2_pipe.fit(X_train, y_train)

  train_scores2.append(logreg_l2_pipe.score(X_train, y_train))
  test_scores2.append(logreg_l2_pipe.score(X_test, y_test))

{c:score for c, score in zip(c_values2, test_scores2)}

fig, ax = plt.subplots(1,1)
ax.plot(c_values2, train_scores2, label='Training Accuracy')
ax.plot(c_values2, test_scores2, label='Testing Accuracy')
ax.set_xticks(c_values2)
ax.set_title('Change in accuracy over C values for l2 regularization')
ax.legend()

ConfusionMatrixDisplay.from_estimator(logreg_l2_pipe, X_test, y_test, cmap = 'Blues');
print("False positive is the top right corner")
print("False Negatives is the Bottom Right Corner")
print("True Positives is the Top Left")
print("True Negatives is the Bottom Right\n\n\n\n")

"""PCA Transformation"""

pca95 = PCA(n_components=.95)

#Without PCA and L2 penalty

# Commented out IPython magic to ensure Python compatibility.
# %%time
# logreg_l2 = LogisticRegression(C=c, max_iter=1000, solver='liblinear', penalty='l2')
# logreg_l2_pipe = make_pipeline(preprocessor, logreg)
# logreg_l2_pipe.fit(X_train, y_train)

logregPreds = logreg_l2_pipe.predict(X_test)
logregPredsAcc = logreg_l2_pipe.score(X_test, y_test)
print(logregPredsAcc)

#With PCA and L2 Penalty

# Commented out IPython magic to ensure Python compatibility.
# %%time
# logreg_l2 = LogisticRegression(C=c, max_iter=1000, solver='liblinear', penalty='l2')
# logreg_l2_pipe_pca = make_pipeline(preprocessor,pca95, logreg)
# logreg_l2_pipe_pca.fit(X_train, y_train)

pcalogregPreds = logreg_l2_pipe_pca.predict(X_test)
pcalogregPredsAcc = logreg_l2_pipe_pca.score(X_test, y_test)
print(pcalogregPredsAcc)