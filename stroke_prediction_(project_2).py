# -*- coding: utf-8 -*-
"""Stroke Prediction (Project 2).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1zbPb8v32u2zOSRsslcnwwYBxhHg20TLR
"""
import pandas as pd
#import numpy as np
import matplotlib.pyplot as plt

#Seaborn
import seaborn as sns
#from seaborn import heatmap
 
# Keras & TensorFlow
#import tensorflow
#from tensorflow import keras as keras
#from keras.models import Sequential
# from keras.layers import Dense, Dropout
# from keras.callbacks import EarlyStopping
# from keras import metrics
 
# Sci-kit learn
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\
    , classification_report, ConfusionMatrixDisplay
from sklearn.preprocessing import StandardScaler, OneHotEncoder, MinMaxScaler
from sklearn.compose import make_column_transformer, make_column_selector
from sklearn.model_selection import train_test_split
from sklearn.impute import SimpleImputer
from sklearn.pipeline import make_pipeline

#KMeans Clustering 

from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
from sklearn.linear_model import LogisticRegression
#from sklearn.model_selection import GridSearchCV
from sklearn.metrics import accuracy_score, recall_score, precision_score, \
f1_score, classification_report, confusion_matrix
from sklearn.metrics import ConfusionMatrixDisplay, RocCurveDisplay
from sklearn import metrics
from sklearn.ensemble import GradientBoostingClassifier

stroke = pd.read_csv('C:/Users/BEEMO/Documents/GitHub/Stroke/healthcare-dataset-stroke-data (2).csv')
df = pd.read_csv('C:/Users/BEEMO/Documents/GitHub/Stroke/healthcare-dataset-stroke-data (2).csv')
stroke.head(2)
stroke.info()
stroke.value_counts()

"""1. Kaggle (https://www.kaggle.com/datasets/fedesoriano/stroke-prediction-dataset)

2. The Data includes statistics of patients who have had a stroke, stroke-like symptoms, and no stroke to determine weather we can predcit someone can have a stroke depending on the other features like bmi, height, weight, age, gender, diseases, etc...
3. Target = Stroke (or not)
4. Classification problem =  whether player has a stroke or not
5&6. There are around 12 features and around 8K rows of data
7. I belive that this dataset is the perfect balance between complex and simplistic allowing for easy predictions and modeling.
"""

#getting a value of all NaN or Missing
stroke.isna().sum()
#Getting all the duplicated values
stroke.duplicated().sum()
#Checking the column 'bmi' because it was the only one contaning missing values
stroke['bmi']
#stroke.drop(stroke.loc[stroke['gender']== 'Other'].index, inplace=True)
#stroke = stroke[stroke.'gender' != 'Other']
stroke = stroke[stroke["gender"].str.contains("Other") == False]
stroke['gender'].value_counts()

nominal_selector = stroke.select_dtypes( include='object')
for col in nominal_selector.columns:
  print(col)
  print(nominal_selector[col].value_counts(), '\n')

stroke['ever_married'].value_counts()
stroke.describe()
Stroke_graph = stroke.hist(figsize = (10,10))

"""Average Glucose level is one of the few abnormal distributions - this one is skewed to the right. BMI is also a histogram that poses a slight skew to the right. Stroke, Heart Disease and Hypertension are all bimodal since the columns are """

plt.style.use('seaborn')
stroke.plot(kind='box',subplots=True ,layout=(3,3),sharex=False , sharey=False , figsize =(10,10))
plt.show()

plt.scatter(stroke.index, stroke['bmi'])
plt.show()

stroke['stroke'].value_counts().plot(kind='bar')

#Creating a correlation map to show moderate to strong correlations between target features and other columns
corr = stroke.corr()
sns.heatmap(corr, cmap = 'Blues')

#Creating a filter to select all columns that are non-numerical so we can correct for inconsistencies
nominal_selector = stroke.select_dtypes(include='object')
for col in nominal_selector.columns:
  print(col)
  print(nominal_selector[col].value_counts(), '\n');

"""Validation Split - Classification Problem"""
#Assigning and splitting target column to X and y with random state of 42
X=stroke.drop(columns='stroke')
y = stroke['stroke']
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)
nominal_selector = make_column_selector(dtype_include='object')
numeric_selector = make_column_selector(dtype_include='number')
#Creating our imputer for later inputing them into our preprocessor and pipeline
scaler = StandardScaler()
ohe = OneHotEncoder(sparse=False, handle_unknown='ignore')
#Initializing and imputing missing values to our data
mean_imputer = SimpleImputer(strategy='mean')
missing_imputer = SimpleImputer(strategy='constant', fill_value='Missing')
#Creating our pipeline
nominal_pipeline = make_pipeline(missing_imputer, ohe)
numeric_pipeline = make_pipeline(mean_imputer, scaler)
nominal_tup = (nominal_pipeline, (['gender', 'ever_married', 'work_type', 'Residence_type', 'smoking_status']))
numeric_tup = (numeric_pipeline, numeric_selector)
#preprocessing the data
preprocessor = make_column_transformer(numeric_tup, nominal_tup, remainder='passthrough')
#Fitting the preprocessor
preprocessor.fit(X_train)

Stroke_graph2 = X_train.hist(figsize = (10,10))

stroke_answer = stroke['stroke'].value_counts()
stroke_label = ['No Stroke','Stroke']
fig, ax = plt.subplots()
ax.pie(stroke_answer, labels=stroke_label, autopct='%1.2f%%',
        shadow=False, startangle=15, counterclock=False)
ax.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.
ax.set_title('Stroke Distribution')
plt.show()

"""The number of people who have suffered from stroke are very minimal compared to those who have had a stroke. """

stroke["age"].nunique()

def age_groups(age):
    if age >= 75:
        return('+75 ')
    elif age > 65:
        return('66 - 75 ')
    elif age > 55:
        return('56 - 65 ')
    elif age > 45:
        return('46 - 55 ')
    elif age > 35:
        return('36 - 45 ')
    elif age > 25:
        return('26 - 35 ')
    elif age > 18:
        return('19 - 25 ')
    elif age > 0:
        return('0 - 18 ')
    else:
        return(None)

stroke['Age_group'] = stroke['age'].apply(age_groups)

sns.countplot(data=stroke,x='gender',hue='age')

#importing necessary libraries


#Creating a function to evaluate metrics towards prediction

def evaluate_classification(model, X_test, y_test, cmap='Greens',
                            normalize=None, classes=None, figsize=(20,5)):
  """Takes as arguments: a model, features, and labels
  Prints a classification report, confusion matrix
  Optional arguments: 
    cmap: colormap 
    normalize: confusion matrix normalization ['true', 'pred', 'all' or None]
    classes: ordered list of class labels
    figsize: size of figure"""
    
  test_preds = model.predict(X_test)
  print(metrics.classification_report(y_test, test_preds, target_names=classes))
  
  ConfusionMatrixDisplay.from_estimator(model, X_test, y_test, cmap=cmap, 
                                display_labels=classes,
                                normalize=normalize)
  plt.show()

X.info()

"""                     Logistic Regression"""

logreg = LogisticRegression()
logreg_pipe = make_pipeline(preprocessor, logreg)
logreg_pipe.fit(X_train, y_train)

train = "Training Score:"
print(f'Training Score:', logreg_pipe.score(X_train, y_train))
print(f'Testing Score:', logreg_pipe.score(X_test, y_test))

#L1 Tunning
c_valuesl1 = [0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000]
train_scoresl1 = []
test_scoresl1 = []
for c in c_valuesl1: 
  logregl1 = LogisticRegression(C=c, max_iter=1000, solver='liblinear', penalty='l1')
  logregl1_pipe = make_pipeline(preprocessor, logregl1)
  logregl1_pipe.fit(X_train, y_train)
  train_scoresl1.append(logregl1_pipe.score(X_train, y_train))
  test_scoresl1.append(logregl1_pipe.score(X_test, y_test))
{c:score for c, score in zip(c_valuesl1, test_scoresl1)}

fig, ax = plt.subplots(1,1)
ax.plot(c_valuesl1, train_scoresl1, label='Training Accuracy')
ax.plot(c_valuesl1, test_scoresl1, label='Testing Accuracy')
ax.set_xticks(c_valuesl1)
ax.set_title('Change in accuracy over C values for l1 regularization')
ax.legend()

#L2 Tunning
c_values2 = [0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000]
train_scores2 = []
test_scores2 = []
for c in c_values2: 
  logreg_l2 = LogisticRegression(C=c, max_iter=1000, solver='liblinear', penalty='l2')
  logreg_l2_pipe = make_pipeline(preprocessor, logreg)
  logreg_l2_pipe.fit(X_train, y_train)

  train_scores2.append(logreg_l2_pipe.score(X_train, y_train))
  test_scores2.append(logreg_l2_pipe.score(X_test, y_test))

{c:score for c, score in zip(c_values2, test_scores2)}

fig, ax = plt.subplots(1,1)
ax.plot(c_values2, train_scores2, label='Training Accuracy')
ax.plot(c_values2, test_scores2, label='Testing Accuracy')
ax.set_xticks(c_values2)
ax.set_title('Change in accuracy over C values for l2 regularization')
ax.legend()

ConfusionMatrixDisplay.from_estimator(logreg_l2_pipe, X_test, y_test, cmap = 'Blues');
print("False positive is the top right corner")
print("False Negatives is the Bottom Right Corner")
print("True Positives is the Top Left")
print("True Negatives is the Bottom Right\n\n\n\n")




"""             KMeans - Clustering             """

#Clustering 
df.dropna(inplace=True)
x = df[['age','avg_glucose_level', 'bmi']]

x.head()

x.describe()

scaled_x = StandardScaler().fit_transform(x)
scaled_x[:5]

ks = range(2,10)
inertia = []

for k in ks:
  model = KMeans(n_clusters=k, random_state=42)
  model.fit(scaled_x)
  inertia.append(model.inertia_)

plt.plot(ks, inertia)
plt.xlabel('# of clusters')
plt.ylabel('inertias')

ks = range(2,10)
sills = []

for k in ks:
  model = KMeans(n_clusters=k, random_state=42)
  model.fit(scaled_x)
  sills.append(silhouette_score(scaled_x, model.labels_))

plt.plot(ks, sills)
plt.xlabel('# of clusters')
plt.ylabel('sills')
plt.show()

#opitmal ~ 3 
best_model = KMeans(n_clusters=3, random_state=42)
y_preds = best_model.fit(scaled_x)
plt.scatter(scaled_x[:,0], 
            scaled_x[:,1])
plt.scatter(best_model.cluster_centers_[:, 0], 
            best_model.cluster_centers_[:, 1], 
            s=200,                             
            # Set centroid size
            c='red')                           
            # Set centroid color
plt.show()

"""In this graph we can see the centroids and the various clusteres mapped upon age, bmi and average glucose level. We can see in the Elbow and the Intertia plot that the highest and optimal number of cluster to groups the people in tihs dataset was 3 as represented inn the scatterplot above. """

cluster_df = df.copy()
cluster_df['cluster'] = best_model.labels_
cluster_df

cluster_groups = cluster_df.groupby('bmi', as_index=False)
cluster_mean = cluster_groups.mean()
print(cluster_mean)

cluster_mean.plot(subplots=True, kind='bar', figsize= (40, 40), sharex=False)
